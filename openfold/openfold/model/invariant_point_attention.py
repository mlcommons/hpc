# Copyright 2021 DeepMind Technologies Limited
# Copyright 2022 AlQuraishi Laboratory
# Copyright 2023 NVIDIA CORPORATION
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import math

import torch
import torch.nn as nn
import torch.nn.functional as F

from openfold.model.linear import Linear
from openfold.rigid_utils import Rigid
from openfold.torch_utils import is_autocast_fp16_enabled


class InvariantPointAttention(nn.Module):
    """Invariant Point Attention (IPA) module.

    Supplementary '1.8.2 Invariant point attention (IPA)': Algorithm 22.

    Args:
        c_s: Single representation dimension (channels).
        c_z: Pair representation dimension (channels).
        c_hidden: Hidden dimension (channels).
        num_heads: Number of attention heads.
        num_qk_points: Number of query/key points.
        num_v_points: Number of value points.
        inf: Safe infinity value.
        eps: Epsilon to prevent division by zero.

    """
    def __init__(
        self,
        c_s: int,
        c_z: int,
        c_hidden: int,
        num_heads: int,
        num_qk_points: int,
        num_v_points: int,
        inf: float,
        eps: float,
    ) -> None:
        super(InvariantPointAttention, self).__init__()
        # configuration:
        self.c_s = c_s
        self.c_z = c_z
        self.c_hidden = c_hidden
        self.num_heads = num_heads
        self.num_qk_points = num_qk_points
        self.num_v_points = num_v_points
        self.inf = inf
        self.eps = eps
        # submodules:
        # These linear layers differ from their specifications in the supplement.
        # There, they lack bias and use Glorot initialization.
        # Here as in the official source, they have bias and use the default Lecun initialization.
        hc = c_hidden * num_heads
        self.linear_q = Linear(c_s, hc, bias=True, init="default")
        self.linear_kv = Linear(c_s, 2 * hc, bias=True, init="default")

        hpq = num_heads * num_qk_points * 3
        self.linear_q_points = Linear(c_s, hpq, bias=True, init="default")

        hpkv = num_heads * (num_qk_points + num_v_points) * 3
        self.linear_kv_points = Linear(c_s, hpkv, bias=True, init="default")

        self.linear_b = Linear(c_z, num_heads, bias=True, init="default")

        self.head_weights = nn.Parameter(torch.zeros((num_heads)))
        ipa_point_weights_init_(self.head_weights.data)

        concat_out_dim = num_heads * (c_z + c_hidden + num_v_points * 4)
        self.linear_out = Linear(concat_out_dim, c_s, bias=True, init="final")

    def forward(
        self,
        s: torch.Tensor,     # [batch, N_res, c_s] single representation
        z: torch.Tensor,     # [batch, N_res, N_res, c_z] pair representation
        r: Rigid,            # [batch, N_res] rigids transformation
        mask: torch.Tensor,  # [batch, N_res] sequence mask
    ) -> torch.Tensor:       # [batch, N_res, c_s] single representation update

        #######################################
        # Generate scalar and point activations
        #######################################

        q = self.linear_q(s)
        # q: [batch, N_res, num_heads * c_hidden]

        kv = self.linear_kv(s)
        # kv: [batch, N_res, num_heads * 2 * c_hidden]

        q = q.view(q.shape[:-1] + (self.num_heads, self.c_hidden))
        # q: [batch, N_res, num_heads, c_hidden]

        kv = kv.view(kv.shape[:-1] + (self.num_heads, -1))
        # kv: [batch, N_res, num_heads, 2 * c_hidden]

        k, v = torch.split(kv, self.c_hidden, dim=-1)
        # k: [batch, N_res, num_heads, c_hidden]
        # v: [batch, N_res, num_heads, c_hidden]

        q_pts = self.linear_q_points(s)
        # q_pts: [batch, N_res, num_heads * num_qk_points * 3]

        q_pts = torch.split(q_pts, q_pts.shape[-1] // 3, dim=-1)
        q_pts = torch.stack(q_pts, dim=-1)
        q_pts = r.unsqueeze(-1).apply(q_pts)
        # q_pts: [batch, N_res, num_heads * num_qk_points, 3]

        q_pts = q_pts.view(q_pts.shape[:-2] + (self.num_heads, self.num_qk_points, 3))
        # q_pts: [batch, N_res, num_heads, num_qk_points, 3]

        kv_pts = self.linear_kv_points(s)
        # kv_pts: [batch, N_res, num_heads * (num_qk_points + num_v_points) * 3]

        kv_pts = torch.split(kv_pts, kv_pts.shape[-1] // 3, dim=-1)
        kv_pts = torch.stack(kv_pts, dim=-1)
        kv_pts = r[..., None].apply(kv_pts)
        # kv_pts: [batch, N_res, num_heads * (num_qk_points + num_v_points), 3]

        kv_pts = kv_pts.view(kv_pts.shape[:-2] + (self.num_heads, self.num_qk_points + self.num_v_points, 3))
        # kv_pts: [batch, N_res, num_heads, (num_qk_points + num_v_points), 3]

        k_pts, v_pts = torch.split(kv_pts, (self.num_qk_points, self.num_v_points), dim=-2)
        # k_pts: [batch, N_res, num_heads, num_qk_points, 3]
        # v_pts: [batch, N_res, num_heads, num_v_points, 3]

        ##########################
        # Compute attention scores
        ##########################

        b = self.linear_b(z)
        # b: [batch, N_res, N_res, num_heads]

        if is_autocast_fp16_enabled():
            with torch.cuda.amp.autocast(enabled=False):
                a = torch.matmul(
                    q.float().movedim(-3, -2),  # q: [batch, num_heads, N_res, c_hidden]
                    k.float().movedim(-3, -1),  # k: [batch, num_heads, c_hidden, N_res]
                )
        else:
            a = torch.matmul(
                q.movedim(-3, -2),  # q: [batch, num_heads, N_res, c_hidden]
                k.movedim(-3, -1),  # k: [batch, num_heads, c_hidden, N_res]
            )
        # a: [batch, num_heads, N_res, N_res]
        a = a * math.sqrt(1.0 / (3 * self.c_hidden))
        a = a + (math.sqrt(1.0 / 3) * b.movedim(-1, -3))
        # a: [batch, num_heads, N_res, N_res]

        pt_att = q_pts.unsqueeze(-4) - k_pts.unsqueeze(-5)  # outer subtraction
        pt_att = pt_att**2
        # pt_att: [batch, N_res, N_res, num_heads, num_qk_points, 3]

        pt_att = sum(torch.unbind(pt_att, dim=-1))
        # pt_att: [batch, N_res, N_res, num_heads, num_qk_points]

        head_weights = F.softplus(self.head_weights)
        head_weights = head_weights.view((1,) * (pt_att.ndim - 2) + (self.num_heads, 1))
        head_weights = head_weights * math.sqrt(1.0 / (3 * (self.num_qk_points * 9.0 / 2)))
        # head_weights: [1, 1, 1, num_heads, 1]

        pt_att = pt_att * head_weights
        # pt_att: [batch, N_res, N_res, num_heads, num_qk_points]

        pt_att = -0.5 * torch.sum(pt_att, dim=-1)
        # pt_att: [batch, N_res, N_res, num_heads]

        square_mask = mask.unsqueeze(-1) * mask.unsqueeze(-2)  # outer product
        square_mask = (square_mask - 1.0) * self.inf
        # square_mask: [batch, N_res, N_res]

        pt_att = pt_att.movedim(-1, -3)
        # square_mask: [batch, num_heads, N_res, N_res]

        a = a + pt_att
        # a: [batch, num_heads, N_res, N_res]

        a = a + square_mask.unsqueeze(-3)
        # a: [batch, num_heads, N_res, N_res]

        a = torch.softmax(a, dim=-1)
        # a: [batch, num_heads, N_res, N_res]

        ################
        # Compute output
        ################

        o = torch.matmul(a, v.transpose(-2, -3).to(dtype=a.dtype)).transpose(-2, -3)
        # o: [batch, N_res, num_heads, c_hidden]

        o = o.reshape(o.shape[:-2] + (self.num_heads * self.c_hidden,))
        # o: [batch, N_res, num_heads * c_hidden]

        o_pt = torch.sum(
            (
                a.unsqueeze(-3).unsqueeze(-1)
                * v_pts.swapdims(-4, -3).movedim(-1, -3).unsqueeze(-3)
            ),
            dim=-2,
        )
        # o_pt: [batch, num_heads, 3, N_res, num_v_points]

        o_pt = o_pt.movedim(-3, -1).swapdims(-3, -4)
        # o_pt: [batch, N_res, num_heads, num_v_points, 3]

        o_pt = r.unsqueeze(-1).unsqueeze(-2).invert_apply(o_pt)
        # o_pt: [batch, N_res, num_heads, num_v_points, 3]

        o_pt_norm = torch.sqrt(torch.sum(o_pt**2, dim=-1) + self.eps)
        o_pt_norm = o_pt_norm.reshape(o_pt_norm.shape[:-2] + (self.num_heads * self.num_v_points,))
        # o_pt_norm: [batch, N_res, num_heads * num_v_points]

        o_pt = o_pt.reshape(o_pt.shape[:-3] + (self.num_heads * self.num_v_points, 3))
        # o_pt: [batch, N_res, num_heads * num_v_points, 3]

        o_pair = torch.matmul(a.transpose(-2, -3), z.to(dtype=a.dtype))
        # o_pair: [batch, N_res, num_heads, c_z]

        o_pair = o_pair.reshape(o_pair.shape[:-2] + (self.num_heads * self.c_z,))
        # o_pair: [batch, N_res, num_heads * c_z]

        o_cat = (o, *torch.unbind(o_pt, dim=-1), o_pt_norm, o_pair)
        o_cat = torch.cat(o_cat, dim=-1)
        # o_cat: [batch, N_res, num_heads * (c_hidden + num_v_points * 4 + c_z)]

        s = self.linear_out(o_cat.to(dtype=z.dtype))
        # s: [batch, N_res, c_s]

        return s


def ipa_point_weights_init_(weights_data: torch.Tensor) -> None:
    softplus_inverse_1 = 0.541324854612918
    weights_data.fill_(softplus_inverse_1)
